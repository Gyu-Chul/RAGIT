# **RAG\_Worker: RAGIT의 핵심 기능 처리 시스템**

Rag\_Worker는 GitHub repository의 소스 코드를 자동으로 분석하고 저장한 뒤, 사용자의 자연어 질문을 관련된 코드를 찾아 자동으로 더 나은 프롬프트로 질문하여 향상된 품질의 답을 얻을 수 있도록 돕는 지능형 Q\&A 시스템입니다. 전체 프로세스는 **검색 증강 생성(RAG, Retrieval-Augmented Generation)** 아키텍처를 기반으로 하며, 모든 작업들은 Celery를 통해 비동기적으로 처리됩니다.

## **핵심 아키텍처 및 동작 흐름**

Rag\_Worker의 전체 파이프라인은 **데이터 준비(Indexing)** 단계와 **질의응답(Querying)** 단계로 나뉩니다. 모든 과정은 API 요청에 의해 트리거되며, Celery 워커들이 백그라운드에서 실제 작업을 수행하고 데이터베이스에 진행 상태를 기록합니다.

### **1단계: 저장소 분석 및 벡터화 (Indexing Pipeline)**

사용자가 GitHub 저장소 URL을 입력하면, 시스템은 코드를 이해하고 검색할 수 있는 형태로 가공하는 자동화된 파이프라인을 시작합니다.

#### **1\. 코드 가져오기 (Git Service)**

* **동작**: 사용자가 제공한 URL을 이용해 **Git Service** 가 해당 저장소를 로컬 파일 시스템으로 안전하게 복제(clone)합니다.  
* **결과**: 원본 코드베이스가 서버에 준비됩니다. 동시에, 이 저장소의 벡터 데이터가 저장될 **Milvus Collection** 이 생성됩니다.

#### **2\. 코드 구조 분석 및 조각화 (Python Parser)**

* **동작**: **Python Parser** 가 복제된 저장소 내의 모든 .py 파일을 탐색합니다. **추상 구문 트리(AST, Abstract Syntax Tree)** 분석 기술을 통해 각 파일을 의미 있는 단위(클래스, 함수, 모듈 레벨 코드 등)로 조각냅니다.  
* **결과**: 원본 코드는 구조화된 메타데이터(파일 경로, 정의 타입, 이름, 코드 라인 등)를 포함한 수많은 JSON 객체(청크)로 분해되어 parsed\_repository 폴더에 저장됩니다.

#### **3\. 벡터 임베딩 및 저장 (Vector DB Service)**

* **동작**: **Vector DB Service** 가 파싱된 JSON 청크들을 읽어옵니다. 각 코드 청크의 텍스트 내용은 Hugging Face 언어 모델을 통해 **의미를 나타내는 벡터**(**Dense Vector**)와 **키워드를 나타내는 벡터**(**Sparse Vector**)로 변환됩니다.  
* **배포 환경 (CPU/GPU 분리)**: 이 벡터 임베딩 과정은 대규모의 계산을 필요로 하는 RAGIT의 핵심 연산입니다. 사용자의 하드웨어 환경에 맞춰 최적의 성능을 제공하기 위해, NVIDIA GPU의 CUDA 가속을 사용하는 버전과 CPU만 사용하는 버전의 도커 이미지가 별도로 분리되어 제공됩니다. GPU 버전은 임베딩 속도를 수십 배 향상시킬 수 있습니다.
* **결과**: 변환된 벡터들은 코드의 메타데이터와 함께 Milvus 벡터 DB의 해당 Collection에 삽입됩니다. 이로써 코드를 의미 기반 및 키워드 기반으로 검색할 수 있는 준비가 완료됩니다.

### **2단계: 질의응답 및 답변 생성 (RAG Pipeline)**

저장소 분석이 완료되면, 사용자는 자연어 질문을 통해 코드와 관련된 정보를 얻을 수 있습니다.

#### **4\. 지능형 코드 검색 (Vector DB Service)**

* **동작**: 사용자의 질문 역시 의미적 유사성을 찾는 **Dense Vector**와 키워드 일치성을 찾는 **Sparse Vector**로 동시에 변환됩니다. Milvus DB는 이 두 벡터를 사용해 **하이브리드 검색**을 수행하며, 각각의 결과는 **RRF(Reciprocal Rank Fusion)** 랭커에 의해 지능적으로 결합되어 최종 순위가 결정됩니다.
* **결과**: 질문에 대한 답변의 근거가 될 가장 관련성 높은 코드 조각(컨텍스트) 리스트를 확보합니다.

#### **5\. 컨텍스트 기반 프롬프트 생성 (Prompt Generator)**

* **동작**: **Prompt Generator** 가 사용자의 원본 질문과 방금 검색된 코드 조각(컨텍스트)들을 하나의 프롬프트로 지능적으로 결합합니다. 이때 각 코드 조각에는 출처와 관련성 점수 등의 정보가 함께 포함되어 LLM이 컨텍스트를 더 잘 이해하도록 돕습니다.  
* **결과**: AI에게 전달할 최종 질문지, 즉 정교하게 설계된 RAG 프롬프트가 생성됩니다.

#### **6\. 최종 답변 생성 및 저장 (LLM Service)**

* **동작**: **LLM Service** 가 완성된 프롬프트를 OpenAI의 GPT 모델(gpt-4o-mini 등) API로 전송합니다. AI는 주어진 코드 컨텍스트 내에서만 답변을 생성하도록 지시받았기 때문에, 코드에 기반한 정확하고 신뢰도 높은 답변을 생성합니다.  
* **결과**: 생성된 최종 답변은 사용자에게 전달되고, 나중에 다시 조회할 수 있도록 Redis 캐시 또는 데이터베이스에 저장됩니다.